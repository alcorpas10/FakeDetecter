{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import numpy\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import datetime\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_general = 'https://maldita.es/malditobulo/1'\n",
    "html_general = requests.get(url_general).text\n",
    "soup_general = BeautifulSoup(html_general, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "566\n"
     ]
    }
   ],
   "source": [
    "clase_nav_bar = 'bg-white text-gray-800 px-3 py-2 text-lg border-gray-400 font-bold'\n",
    "\n",
    "pags_link = soup_general.find('span', attrs={'class' : clase_nav_bar})('a')[0].get('href', None)\n",
    "num_pags = int(pags_link.split('/')[-1])\n",
    "print(num_pags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_characters(text):\n",
    "    \n",
    "    text_aux = ''.join(c for c in unicodedata.normalize('NFD', text.strip())\n",
    "                  if unicodedata.category(c) != 'Mn')\n",
    "    return re.sub(r'[^\\w\\s]', '', text_aux.lower().replace(u'\\xa0', u' ').replace(u'\\n', u' ')).split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_characters_title(text):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', text.strip())\n",
    "                  if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulos = {}\n",
    "\n",
    "clase_new = 'section-card-headline'\n",
    "cont = 0\n",
    "for i in range(1, num_pags+1):\n",
    "    url = 'https://maldita.es/malditobulo/'+str(i)\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    for div in soup.find_all('div',attrs={'class' : clase_new}):\n",
    "        try:\n",
    "            a = div('a')[0]\n",
    "            title = a.text\n",
    "            link = a.get('href', None)\n",
    "            words = dict()\n",
    "            \n",
    "            # Obtener la fecha de la noticia\n",
    "            if \"https://maldita.es/malditobulo/\" in link:\n",
    "                date = link.replace(\"https://maldita.es/malditobulo/\", \"\")[0:8]\n",
    "            elif \"https://migracion.maldita.es/\" in link:\n",
    "                date = link.replace(\"https://migracion.maldita.es/\", \"\")[0:8]\n",
    "            else:\n",
    "                date = link.replace(\"https://alimentacion.maldita.es/mitos-bulos/\", \"\")[0:8]\n",
    "            \n",
    "            formated_date =  datetime.datetime.strptime(date, '%Y%m%d').strftime('%Y/%m/%d)\n",
    "            \n",
    "            html_new = requests.get(link, allow_redirects=False).text\n",
    "\n",
    "            soup_new = BeautifulSoup(html_new, 'html.parser')\n",
    "\n",
    "            title_list = replace_characters(title.lower())\n",
    "\n",
    "\n",
    "            for w in title_list:\n",
    "                if w != '':\n",
    "                    if w in words:\n",
    "                        words[w] += 1\n",
    "                    else:\n",
    "                        words[w] = 1\n",
    "\n",
    "                    \n",
    "            new_text = soup_new.find('div',attrs={'id' : \"article-text\"})            \n",
    "            \n",
    "            text_list = new_text('p') + new_text('li')\n",
    "\n",
    "            \n",
    "            for text in text_list:\n",
    "                if(not text.get_text().startswith(\"Una publicaci√≥n compartida de\")):\n",
    "                    for w in replace_characters(text.get_text()):\n",
    "                        if w != '':\n",
    "                            if w in words:\n",
    "                                words[w] += 1\n",
    "                            else:\n",
    "                                words[w] = 1\n",
    "\n",
    "\n",
    "            bulos[cont] = {\"titulo\" : replace_characters_title(title), \"link\" : link, \"date\" : formated_date, \"words_count\" : words}\n",
    "        except:\n",
    "            print(\"No se ha podido obtener la informacion de la noticia\", i)\n",
    "        cont += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bulos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fakenewsMaldita.json', 'w') as fp:\n",
    "    json.dump(bulos, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
